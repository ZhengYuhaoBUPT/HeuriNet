# 🧠 DDPG算法简介（Deep Deterministic Policy Gradient）

## 📌 算法简介

DDPG（Deep Deterministic Policy Gradient）是一种**基于 actor-critic 架构的确定性策略**深度强化学习算法，主要用于**连续动作空间**的强化学习任务。

该算法结合了：
- DQN 中的目标网络与经验回放机制
- Actor-Critic 架构中的策略梯度方法

> DDPG 是一种离策略（off-policy）算法，能有效利用历史经验数据进行学习。

---

## 🏗️ 模型架构

DDPG包含以下四个主要神经网络：
- 🎭 Actor 网络：给定状态 \( s \)，输出动作 \( a \)
- 🧠 Critic 网络：输入状态-动作对 \( (s, a) \)，输出 Q 值估计
- 🎯 Target Actor 网络：Actor 的延迟更新版本
- 🧮 Target Critic 网络：Critic 的延迟更新版本


---

## 🌟 核心思想与优势

1. **适用于连续动作空间**：不像DQN只能处理离散动作，DDPG可处理高维、连续动作问题。
2. **利用目标网络提高稳定性**：通过延迟更新目标网络参数减少训练震荡。
3. **离策略学习 + 经验回放**：可反复利用数据，提高样本效率。
4. **确定性策略**：输出具体动作而非概率分布，适用于确定性控制任务。

---

## ⚖️ 与其他DRL算法对比

| 算法 | 类型 | 是否支持连续动作 | 样本利用效率 | 稳定性 |
|------|------|------------------|---------------|--------|
| **DDPG** | Actor-Critic，确定性策略 | ✅ 是 | 中等 | 一般 |
| DQN   | 值迭代，离散动作 | ❌ 否 | 高 | 高（离散动作） |
| PPO   | 策略优化，随机策略 | ✅ 是 | 低 | 高 |
| SAC   | 最大熵策略，随机策略 | ✅ 是 | 高 | 高 |

🔍 **为什么其他算法不如DDPG？**
- DQN 无法处理连续动作。
- PPO 是在策略算法，样本利用率低。
- 虽然 SAC 在稳定性和样本效率方面优于 DDPG，但DDPG结构更简单、训练更快，适用于资源受限的环境或小型问题。

---

## 📦 安装依赖
```bash
pip install torch gym numpy
