# 深度强化学习算法：Soft Actor-Critic（SAC）

## 📌 简介

Soft Actor-Critic（SAC）是一种基于最大熵强化学习的离策略算法，它结合了策略梯度和Q学习的优点，并通过引入熵正则项来提升策略的探索能力与鲁棒性。

相比于 DDPG、TD3 等传统离策略算法，SAC 在**收敛速度**、**稳定性**和**样本效率**上都有显著优势，特别适合连续动作空间的强化学习任务。

---

## 🧠 模型架构

SAC 使用 **actor-critic 架构**，并引入了以下组件：

- ✅ **策略网络（Actor）**：输出一个高斯分布，用于从中采样连续动作。
- ✅ **两个Q网络（Critic）**：分别估计同一状态-动作对的值，以减少过估计偏差。
- ✅ **值函数网络（可选）**：在旧版本SAC中存在，后期版本多将其省略。
- ✅ **熵项自动调节机制**：根据目标熵自动调整温度系数 α，无需手动调参。

---

## 🚀 SAC的优势

- ✅ **最大熵目标**：不仅最大化奖励，还鼓励策略具有更高的不确定性（探索性强）。
- ✅ **离策略更新**：可以重用经验回放池中的样本，提高样本利用率。
- ✅ **双Q网络机制**：减少Q值的过估计，训练更稳定。
- ✅ **自适应温度系数 α**：自动调整探索与利用的权衡，提升训练鲁棒性。
- ✅ **收敛速度快，效果强稳**：在连续控制任务（如Mujoco）中表现突出。

---

## ❌ 其他算法的不足对比

| 算法     | 缺点说明 |
|----------|-----------|
| **DDPG** | 策略是确定性的，探索能力差，容易陷入局部最优；训练不稳定。 |
| **TD3**  | 尽管引入了延迟更新与双重Q网络，但没有熵正则，探索依赖外部噪声。 |
| **PPO**  | 虽然稳定性高，但为**在策略算法**，样本利用率低，训练成本高。 |
| **SAC**  | 训练稳定、样本高效、探索能力强，综合性能优于上述方法。 |

---

## 📦 依赖环境

```bash
pip install torch gym numpy
