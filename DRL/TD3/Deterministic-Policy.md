# 🎯 确定性策略（Deterministic Policy）简介

在深度强化学习中，策略用于指导智能体在给定状态下应采取什么动作。策略可以分为两类：

- **确定性策略（Deterministic Policy）**：每个状态始终输出一个固定动作。
- **随机策略（Stochastic Policy）**：每个状态输出一个动作的概率分布。

---

## ✅ 定义

确定性策略形式化地定义为一个函数：
\[
\pi(s) = a
\]
其中：
- \( s \)：当前环境状态
- \( a \)：策略输出的唯一动作

---

## 📌 特点
- 🔹 每个状态\( s \)对应唯一动作 \( a \)  
- 🔹 更适用于连续动作空间问题（如 DDPG、TD3）  
- 🔹 推理速度快，易于部署

---

## ⚠️ 缺点
- ❗ 探索性不足，训练初期可能陷入局部最优  
- ❗ 容易过拟合训练数据，泛化能力弱   
- ❗ 对环境变化不够鲁棒
---

## 🚀 提升方法

为了增强确定性策略的泛化能力和探索性，通常在训练过程中加入噪声：
```bash
action = policy(state)
noise = torch.randn_like(action) * noise_std
action_with_noise = (action + noise).clamp(-max_action, max_action)
```

这种策略被称为带噪确定性策略（Noisy Deterministic Policy），在 TD3 等算法中被广泛应用。

---

## 🔚 总结
确定性策略是处理高维、连续动作空间任务的重要工具。虽然存在过拟合与探索不足的风险，但通过引入噪声、目标网络延迟更新等机制，可以极大提升其表现力与稳定性。


